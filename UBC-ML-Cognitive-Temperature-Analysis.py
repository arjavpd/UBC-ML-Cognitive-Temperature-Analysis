# -*- coding: utf-8 -*-
"""MANU465Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HXD8pNbA_lWa-b9dDiHIhcKXQHoPs-Bz

# Studying the Effects of Ambient Temperature on Cognitive Function
MANU 465  
Ramiro Bolanos, Santiago Mendoza Reyes, Arjav Prasad, Sanskar Soni, Yuhan Zeng

## Introduction

The motivation behind this project is to reveal the effects that different ambient temperatures have on cognitive performance.
Cognitve tests were given to 29 participants in both room temperature conditions (around 22 degrees Celsius) and in cold temperature conditions (around 9 degrees Celsius). Particpants were wearing the Muse headband which measures Electroencephalography (EEG) signals. Using these signals we hope to model this binary classfication problem.

We first created the datasets for test and train by applying Fast Fourier Transform (FFT) on 4 Raw data featues gotten from 4 sensors. We encoded the data, split it, scaled it before applying a variety of models.
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Procedure

### 0. Importing Libraries
"""

import os
import pandas as pd
import numpy as np
from scipy.fft import fft

"""### 1. Data Pre-Proccessing

We apply Fast Fourier Transform (FFT) analyis to convert our time-domain signals to the frequency-domain. We create two datasets. One for Train and one for Test. The following scripts will run FFT analyis on every entry for every 8 seconds.

#### Getting Train Dataset
"""

import warnings
warnings.filterwarnings('ignore')

# Define the base directory containing the 'Room' and 'Cold' folders
base_directory = '/content/drive/MyDrive/MANU465Project/SubmissionDatasets/CleanedData/Train'
modes = ['Room', 'Cold']

# Determine the sampling rate (samples per second)
sampling_rate = 255

# Calculate the number of samples for an 8-second interval
samples_for_8_seconds = 8 * sampling_rate

# Columns to perform FFT on
fft_columns = ['RAW_TP9', 'RAW_AF7', 'RAW_AF8', 'RAW_TP10']

# Initialize the main DataFrame to store all results
train_dataset = pd.DataFrame()

# Iterate over each mode
for mode in modes:
    mode_directory = os.path.join(base_directory, mode)

    # Iterate over each file in the mode directory
    for file in os.listdir(mode_directory):

        # Load the data from the file
        file_path = os.path.join(mode_directory, file)
        data = pd.read_csv(file_path)

        # Initialize a DataFrame to store FFT results for this file
        feature_rows = pd.DataFrame()

        # Iterate over the data in 8-second intervals
        for start in range(0, len(data), samples_for_8_seconds):
            end = start + samples_for_8_seconds
            if end > len(data):
                break  # Skip the last segment if it's shorter than 8 seconds

            # Extract the segment
            segment = data.iloc[start:end]

            # Initialize a temporary DataFrame for the FFT results of this segment
            segment_fft_results = []

            # Perform FFT on each specified column
            for col in fft_columns:
                # Check if the column exists in the segment
                if col in segment.columns:
                    # Applying FFT and handling NaN values
                    transformed_data = fft(np.nan_to_num(segment[col]))
                    # Store the squared magnitude of the FFT result
                    segment_fft_results.extend(np.abs(transformed_data)**2)

            # Append the FFT results of this segment to the feature_rows DataFrame
            feature_rows = feature_rows.append(pd.DataFrame([segment_fft_results]))

        # Adjusting the column names
        feature_rows.columns = [f'{col}_{i}' for col in fft_columns for i in range(samples_for_8_seconds)]

        # Append the mode to each row
        feature_rows['Mode'] = mode

        # Append the results to the main DataFrame
        train_dataset = pd.concat([train_dataset, feature_rows], ignore_index=True)

# Reset the index and optionally save the dataset
train_dataset.reset_index(drop=True, inplace=True)

"""#### Encoding Train Dataset"""

# Replace 'Room' with 0 and 'Cold' with 1 in the 'Mode' column
train_dataset['Mode'] = train_dataset['Mode'].replace({'Room': 0, 'Cold': 1})

train_dataset

"""#### Getting Test Dataset"""

import warnings
warnings.filterwarnings('ignore')

# Define the base directory containing the 'Room' and 'Cold' folders
base_directory = '/content/drive/MyDrive/MANU465Project/SubmissionDatasets/CleanedData/Test'
modes = ['Room', 'Cold']

# Determine the sampling rate (samples per second)
sampling_rate = 255

# Calculate the number of samples for an 8-second interval
samples_for_8_seconds = 8 * sampling_rate

# Columns to perform FFT on
fft_columns = ['RAW_TP9', 'RAW_AF7', 'RAW_AF8', 'RAW_TP10']

# Initialize the main DataFrame to store all results
test_dataset = pd.DataFrame()

# Iterate over each mode
for mode in modes:
    mode_directory = os.path.join(base_directory, mode)

    # Iterate over each file in the mode directory
    for file in os.listdir(mode_directory):

        # Load the data from the file
        file_path = os.path.join(mode_directory, file)
        data = pd.read_csv(file_path)

        # Initialize a DataFrame to store FFT results for this file
        feature_rows = pd.DataFrame()

        # Iterate over the data in 8-second intervals
        for start in range(0, len(data), samples_for_8_seconds):
            end = start + samples_for_8_seconds
            if end > len(data):
                break  # Skip the last segment if it's shorter than 8 seconds

            # Extract the segment
            segment = data.iloc[start:end]

            # Initialize a temporary DataFrame for the FFT results of this segment
            segment_fft_results = []

            # Perform FFT on each specified column
            for col in fft_columns:
                # Check if the column exists in the segment
                if col in segment.columns:
                    # Applying FFT and handling NaN values
                    transformed_data = fft(np.nan_to_num(segment[col]))
                    # Store the squared magnitude of the FFT result
                    segment_fft_results.extend(np.abs(transformed_data)**2)

            # Append the FFT results of this segment to the feature_rows DataFrame
            feature_rows = feature_rows.append(pd.DataFrame([segment_fft_results]))

        # Adjusting the column names
        feature_rows.columns = [f'{col}_{i}' for col in fft_columns for i in range(samples_for_8_seconds)]

        # Append the mode to each row
        feature_rows['Mode'] = mode

        # Append the results to the main DataFrame
        test_dataset = pd.concat([test_dataset, feature_rows], ignore_index=True)

# Reset the index and optionally save the dataset
test_dataset.reset_index(drop=True, inplace=True)

"""#### Encoding Test Dataset"""

# Replace 'Room' with 0 and 'Cold' with 1 in the 'Mode' column
test_dataset['Mode'] = test_dataset['Mode'].replace({'Room': 0, 'Cold': 1})

test_dataset

"""### 2. Splitting the Data

#### Getting X_train
"""

X_train = train_dataset.iloc[:, :-1]
X_train

"""#### Getting y_train"""

y_train = train_dataset['Mode']
y_train

"""#### Getting X_test"""

X_test = test_dataset.iloc[:, :-1]
X_test

"""#### Getting y_test"""

y_test = test_dataset['Mode']
y_test

"""### 3. Scaling the data"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""### 4. Modeling

We tested multipled models and compared the accuracy scores to determine which model best fit our data.
"""

from sklearn.metrics import accuracy_score

"""#### Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

# Creating and training the Decision Tree classifier
dt_classifier = DecisionTreeClassifier(random_state=42, max_depth = 2)
dt_classifier.fit(X_train_scaled, y_train)

# Making predictions
y_pred = dt_classifier.predict(X_test_scaled)

# Evaluating the model
dt_accuracy = accuracy_score(y_test, y_pred)
print(f"Decision Tree Accuracy: {dt_accuracy*100:.2f}%")

"""#### Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# Creating and training the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust the number of trees and other parameters
rf_classifier.fit(X_train_scaled, y_train)

# Making predictions
y_pred = rf_classifier.predict(X_test_scaled)

# Evaluating the model
rf_accuracy = accuracy_score(y_test, y_pred)
print(f"Random Forest Accuracy: {rf_accuracy*100:.2f}%")

"""### XGBoost"""

import xgboost as xgb

# Create the XGBoost classifier
xgb_classifier = xgb.XGBClassifier(objective='binary:logistic', random_state=42, max_depth = 2, n_estimators = 100, learning_rate = 0.3)

# Train the classifier
xgb_classifier.fit(X_train_scaled, y_train)

# Make predictions
y_pred = xgb_classifier.predict(X_test_scaled)

# Evaluate the model
xg_accuracy = accuracy_score(y_test, y_pred)
print(f"XGBoost Accuracy: {xg_accuracy*100:.2f}%")

"""#### Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

# Create a Gaussian Naive Bayes classifier
nb_classifier = GaussianNB()

# Train the classifier
nb_classifier.fit(X_train_scaled, y_train)

# Making predictions
y_pred = nb_classifier.predict(X_test_scaled)

# Evaluating the model
nb_accuracy = accuracy_score(y_test, y_pred)
print(f"Naive Bayes Accuracy: {nb_accuracy*100:.2f}%")

"""### Logistic Regression"""

from sklearn.linear_model import LogisticRegression

# Creating and training the logistic regression model
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)

# Making predictions
y_pred = log_reg.predict(X_test_scaled)

# Evaluating the model
lr_accuracy = accuracy_score(y_test, y_pred)
print(f"Logistic Regression Accuracy: {lr_accuracy*100:.2f}%")

"""#### ANN"""

from keras.models import Sequential
from keras.layers import Dense

# Building the ANN model
model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' for multi-class classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_scaled, y_train, epochs=5, batch_size=32, validation_split=0.2)

# Evaluate the model
loss, ann_accuracy = model.evaluate(X_test_scaled, y_test)
print(f"Test Accuracy: {ann_accuracy*100:.2f}%")

"""#### KNN"""

from sklearn.neighbors import KNeighborsClassifier

# Creating and training the KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# Making predictions
y_pred = knn.predict(X_test_scaled)

# Evaluating the model
knn_accuracy = accuracy_score(y_test, y_pred)
print(f"KNN Accuracy: {knn_accuracy*100:.2f}%")

"""## Summary"""

import matplotlib.pyplot as plt

# List of accuracy values
accuracies = [dt_accuracy, rf_accuracy, xg_accuracy, nb_accuracy, lr_accuracy, ann_accuracy, knn_accuracy]

# List of model names
model_names = ['Decision Tree', 'Random Forest', 'XGBoost', 'Naive Bayes', 'Logistic Regression', 'ANN', 'KNN']

# Convert accuracies to percentages
accuracies_percentage = [accuracy * 100 for accuracy in accuracies]

# Create a bar chart
plt.figure(figsize=(10, 6))
bars = plt.bar(model_names, accuracies_percentage, color='skyblue')
plt.xlabel('Models')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy of Different Models')
plt.ylim(0, 100)  # Set the y-axis limit to 0-100%
plt.xticks(rotation=45)  # Rotate the x-axis labels for better readability

# Add labels in the middle of the bars
for bar, accuracy in zip(bars, accuracies_percentage):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2, f'{accuracy:.2f}%', ha='center', va='center', fontsize=10)

# Show the plot
plt.tight_layout()
plt.show()

"""The Random Forest and XGBoost were the strongest models and provided the highest accuracy. We first tried decision tree and noticed it had a higher accuracy than other models. We then tried out more advanced tree based models like random forest and XGBoost. As expected those models gave higher accuracies than the decision tree model.

Random Forest is an ensemble method that builds multiple Decision Trees and combines their predictions to make more accurate and robust predictions.

XGBoost is also an ensemble method, combining the predictions of multiple base estimators (decision trees, in this case). It uses complex regularization techniques known as regularization trees to prevent overfitting and improve generalization.

## Conclusion
"""

# Making a plot to visualise the fact that the two temperatures can be differentitated from one another. Do this by taking the average of the cold and room temperature rows for the FFT datasets:

# Calculate the average for the specified rows
avg_test_room = X_test.iloc[0:9].mean(axis=1)
avg_test_cold = X_test.iloc[10:17].mean(axis=1)
avg_train_room = X_train.iloc[0:29].mean(axis=1)
avg_train_cold = X_train.iloc[30:57].mean(axis=1)

# Concatenate the room and cold averages
avg_room = pd.concat([avg_test_room, avg_train_room])
avg_cold = pd.concat([avg_test_cold, avg_train_cold])

# Make a scatter plot of the two series of averages
import matplotlib.pyplot as plt

# Plot histograms; alpha is about transparency of the bins (alpha=0 means completely transparent)
plt.hist(avg_room, alpha=0.5, bins=30, label='Room Averages')
plt.hist(avg_cold, alpha=0.5, bins=30, label='Cold Averages')
plt.title('Frequency of Averages of Amplitudes after FFT')
plt.xlabel('Average of Wave Amplitudes after FFT over all Frequencies')
plt.ylabel('Absolute Frequency of Occurrence')
plt.legend()
plt.show()

"""This histogram shows each average obtained above is the average of the amplitudes of the waves that make up brainwaves at all measured frequencies (average of the numbers in the FFT dataset by row).

In room temperature, the absolute frequency of the average amplitude of the waves is high when the average amplitude is about 1.3e9, meaning that in room temperature, the overall magnitude of the brain's activity (brainwaves) is greater as compared with a cold ambient temperature.

In conclusion, we were successful in using machine learning to model brainwave changes during cognitive tasks at different ambient temperatures. There is a clear difference between the brainwave pattern of cold vs room temperature tests.
"""